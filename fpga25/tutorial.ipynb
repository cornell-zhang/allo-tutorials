{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Allo tutorial! We will walk through the basic usage of Allo and some advanced features. We will start with a simple example of matrix multiplication and then gradually optimize it to achieve high performance.\n",
    "\n",
    "Feel free to ask questions during the live demo!\n",
    "\n",
    "Allo is a Python-based Accelerator Design Language (ADL). It is designed to be simple and easy to use, while also providing a powerful set of primitives for hardware customizations.\n",
    "\n",
    "Allo documentation: https://cornell-zhang.github.io/allo/\n",
    "\n",
    "First, we import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Definition\n",
    "Allo leverages an algorithm-customization decoupled paradigm, which means\n",
    "users can first define the algorithm in a high-level language and then\n",
    "optimize the program with various hardware customization techniques (i.e.,\n",
    "schedule primitives). Here we show how to define a general matrix multiplication\n",
    "(GEMM) in the Allo DSL.\n",
    "\n",
    "We first import the necessary data types from Allo. In this example, we\n",
    "use ``float32`` as the data type for all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allo.ir.types import float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function that takes two 128x128 matrices as inputs and\n",
    "returns a 128x128 matrix as output. The variable declaration is defined\n",
    "as ``<name>: <type>[<shape>]``, and the function type is defined as\n",
    "``(<in_type0>, <in_type1>, ...) -> <out_type>``.\n",
    "We require **strict type annotation** in Allo's kernels, which is different\n",
    "from directly programming in Python.\n",
    "\n",
    "Inside the kernel, we provide a shorthand for the loop iterator. For example,\n",
    "``for i, j in allo.grid(128, 128)`` is equivalent to the following\n",
    "nested for-loop:\n",
    "\n",
    "```python\n",
    "    for i in range(128):\n",
    "        for j in range(128):\n",
    "            # body\n",
    "```\n",
    "The ``allo.grid`` API is used to define the iteration space of the loop.\n",
    "The arguments denote the upper bounds of the loop iterators.\n",
    "Notice the above range-loop is also supported in the new Allo, so\n",
    "users have more flexibility to define the loop structure.\n",
    "\n",
    "We also provide ``allo.reduction`` to define the reduction loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, K = 128, 128, 128\n",
    "\n",
    "def gemm(A: float32[M, K], B: float32[K, N]) -> float32[M, N]:\n",
    "    C: float32[M, N] = 0.0\n",
    "    for i, j in allo.grid(M, N):\n",
    "        for k in allo.reduction(K):\n",
    "            C[i, j] += A[i, k] * B[k, j]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Schedule\n",
    "\n",
    "Hardware customizations in Allo are applied on a **schedule**.  After defining the algorithm, we create a schedule with ``allo.customize``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = allo.customize(gemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the Intermediate Representation (IR)\n",
    "Allo leverage the `MLIR <https://mlir.llvm.org/>`_ infrastructure to\n",
    "represent the program, and we can directly print out the IR by using\n",
    "``s.module``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLIR program is\n",
    "a set of operations in different dialects, and the operations are referred\n",
    "to as ``<dialect>.<ops>``. In this example, we can see that the generated IR\n",
    "contains the following dialects:\n",
    "- ``func``: Used to define the function signature and the return of the function.\n",
    "- ``memref``: Used to define the shape and memory layout of the tensors.\n",
    "- ``affine``: Used to define the loop structure.\n",
    "- ``arith``: Used to conduct actual arithmetic operations.\n",
    "- ``linalg``: Currently only used to initialize the tensors.\n",
    "And the inner-most dot-product is explicitly represented by a sequence of load/store\n",
    "operations and some arithmetic operations.\n",
    "Allo also attaches some attributes to the operations, including the tensor\n",
    "names, loop names, and operation names, which are further used for optimization.\n",
    "\n",
    "ðŸ“Œ **Note**: Allo customizations are applied immediately on the IR. In the later exercises, you can print the IR after each customization to see the changes.\n",
    "\n",
    "\n",
    "### Validate the Functional Correctness on the CPU Backend\n",
    "\n",
    "Allo supports multiple backends, including CPU, FPGA, and AI Engine. We can target different backends by specifying the target hardware in the ``.build()`` function. We will start with the CPU backend.\n",
    "\n",
    "For functional validation on the CPU backend, we  call ``.build()`` function on the schedule and specify the target\n",
    "hardware as ``llvm``. By default, Allo will generate a LLVM program that\n",
    "can be executed on the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable = s.build(target=\"llvm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ **Note**: ``s.build(target=\"llvm\")`` is equivalent to ``s.build()``.\n",
    "\n",
    "\n",
    "#### Prepare the Inputs/Outputs for the Executable\n",
    "\n",
    "To run the executable, we can generate random NumPy arrays as input data, and\n",
    "directly feed them into the executable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_A = np.random.rand(M, K).astype(np.float32)\n",
    "np_B = np.random.rand(K, N).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Executable\n",
    "\n",
    "With the prepared inputs/outputs, we can feed them to our executable.\n",
    "Notice our module can return a new array as output, so we can directly\n",
    "assign the output to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_C = executable(np_A, np_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the results with the NumPy to see if the results are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_C = np.matmul(np_A, np_B)\n",
    "np.testing.assert_allclose(np_C, golden_C, rtol=1e-3, atol=1e-3)\n",
    "print(\"\\033[92mResults are correct! âœ…\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target the FPGA Backend\n",
    "\n",
    "To generate high-performance designs for FPGA, we apply hardware-specific customizations to transform algorithm specifications into efficient hardware implementations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Vitis HLS\n",
    "Before delving into the details, let's set up the environment variables to use Vitis HLS in Jupyter notebook. This step is only required for Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "# Run Bash to source the script and print environment variables\n",
    "bash_command = \"bash -c 'source /opt/xilinx/Vitis_HLS/2022.1/settings64.sh && env'\"\n",
    "env_vars = subprocess.run(bash_command, shell=True, capture_output=True, text=True, check=True)\n",
    "\n",
    "# Parse and update Python's environment variables\n",
    "for line in env_vars.stdout.split(\"\\n\"):\n",
    "    if \"=\" in line:\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Verify\n",
    "!which vitis_hls\n",
    "\n",
    "import allo.backend.hls as hls\n",
    "print(hls.is_available(\"vitis_hls\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Baseline Implementation\n",
    "\n",
    "To target FPGA, we simply change the target to ``vitis_hls``. For example, we can specify the mode as ``csyn`` to synthesize the design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = s.build(target=\"vitis_hls\", mode=\"csyn\", project=\"baseline.prj\")\n",
    "mod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a Vivado HLS project in the ``baseline.prj`` directory. You can navigate to the project folder to find the generated `kernel.cpp` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any customizations, the generated design is a inner-product matrix multiply. The following is a simplified version of the generated `kernel.cpp` HLS code and the corresponding datapath diagram.\n",
    "\n",
    "<div style=\"text-align:center\"><img width=90% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1740726173/default_gemm_udtmvl.png\" alt=\"default gemm\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resource utilization and performance results are available in the Vitis HLS report ``baseline.prj/out.prj/solution1/syn/report/gemm_csynth.rpt``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<details><summary markdown=\"span\">Let's see some HLS report!</summary>\n",
    "\n",
    "```text\n",
    "+----------+----------+-----------+-----------+----------+----------+---------+\n",
    "|   Latency (cycles)  |   Latency (absolute)  |       Interval      | Pipeline|\n",
    "|    min   |    max   |    min    |    max    |    min   |    max   |   Type  |\n",
    "+----------+----------+-----------+-----------+----------+----------+---------+\n",
    "|  14958622|  14958622|  49.812 ms|  49.812 ms|  14958623|  14958623|       no|\n",
    "+----------+----------+-----------+-----------+----------+----------+---------+\n",
    "\n",
    "    \n",
    "================================================================\n",
    "== Utilization Estimates\n",
    "================================================================\n",
    "* Summary: \n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|         Name        | BRAM_18K|  DSP |    FF   |   LUT   | URAM|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|DSP                  |        -|     -|        -|        -|    -|\n",
    "|Expression           |        -|     -|        0|      458|    -|\n",
    "|FIFO                 |        -|     -|        -|        -|    -|\n",
    "|Instance             |        0|     5|     3258|     4428|    0|\n",
    "|Memory               |       48|     -|        0|        0|    0|\n",
    "|Multiplexer          |        -|     -|        -|      611|    -|\n",
    "|Register             |        -|     -|      691|        -|    -|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Total                |       48|     5|     3949|     5497|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Available SLR        |     1344|  3008|   869120|   434560|  320|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Utilization SLR (%)  |        3|    ~0|       ~0|        1|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Available            |     4032|  9024|  2607360|  1303680|  960|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Utilization (%)      |        1|    ~0|       ~0|       ~0|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Vitis HLS will automatically apply pipelining to the innermost loop. We can examine the pipeline result in HLS report:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```text\n",
    "* Loop: \n",
    "+-------------+---------+---------+----------+-----------+-----------+------+----------+\n",
    "|             |  Latency (cycles) | Iteration|  Initiation Interval  | Trip |          |\n",
    "|  Loop Name  |   min   |   max   |  Latency |  achieved |   target  | Count| Pipelined|\n",
    "+-------------+---------+---------+----------+-----------+-----------+------+----------+\n",
    "|- l_S_k_0_k  |      902|      902|        14|          7|          1|   128|       yes|\n",
    "+-------------+---------+---------+----------+-----------+-----------+------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the read-after-write dependency on ``k`` loop iterations, it can only be pipelined to an initiation interval of 7, meaning that a new iteration can only start evey 7 cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Customizations\n",
    "\n",
    "One way to improve the pipelining is to move the reduction loop outside, so the innermost loop does not have data dependency.\n",
    "\n",
    "In this section, we exercise single-kernel customizations with an example: transforming an inner-product matrix multiply to scalar-vector product. \n",
    "\n",
    "<div style=\"text-align:center\"><img width=60% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1740145203/svp_ixb7ya.png\" alt=\"scalar-vector\"></div>\n",
    "\n",
    "The figure above illustrates the difference between inner-product and scalar-vector product computations. In the inner-product implementation, the loop over `k` accumulates partial sums. If we attempt to pipeline this loop, we cannot initiate the next iteration immediately after the previous one starts due to data dependencies in the accumulation process.\n",
    "\n",
    "Instead, we can reorder loops `k` and `j` and pipeline loop `j`. Since there are no dependencies between iterations of loop `j`, a new iteration can begin every cycle. This effectively transforms the computation into a scalar-vector product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to reorder the inner reduction loop with the middle loop. This is for changing the computation order of matrix multiply.\n",
    "\n",
    "**Exercise**: reorder loop `k` and loop `j` with the `.reorder()` primitive.\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "reorder(*args)\n",
    "# Reorders nested loops with indices listed in args such that the outermost loop is the first index listed in args, the second is the second outermost, and so on.\n",
    "```\n",
    "\n",
    "ðŸ’¡**Tip**: You can print the IR after each customization to see the changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "  <summary> Answer </summary>\n",
    "  \n",
    "  `s.reorder(\"k\", \"j\")`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need an accumulation buffer for one row of partial sums. We create a new buffer for the output tensor C. We provide a `.buffer_at(tensor, axis=\"loop\")` primitive for users to quickly create a new buffer along a specific axis. Since Allo has attached all the tensors to the function, we can directly use <schedule>.<tensor> to access a specific tensor in the schedule.\n",
    "\n",
    "**Exercise**: insert a buffer for output tensor `C` at loop level `i`.\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "buffer_at(target, axis)\n",
    "# Creates a chip buffer to hold the values of target written to in loop with index axis instead of immediately writing them to memory.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "  <summary> Answer </summary>\n",
    "  \n",
    "  `s.buffer_at(s.C, axis=\"i\")`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lastly, we pipeline the `j` loop in order to achieve the best performance.\n",
    "\n",
    "**Exercise**: pipeline loop `j`.\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "pipeline(axis[, initiation_interval, rewind])\n",
    "# Pipelines a loop with index axis into initiation_interval stages.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "  <summary> Answer </summary>\n",
    "  \n",
    "  `s.pipeline(\"j\")`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's push the design through synthesis and observe the speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = s.build(target=\"vitis_hls\", mode=\"csyn\", project=\"scalar-vector.prj\")\n",
    "mod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the generated `kernel.cpp` HLS code and the corresponding datapath diagram in the `scalar-vector.prj` directory. The HLS report is available in `scalar-vector.prj/out.prj/solution1/syn/report/gemm_csynth.rpt`.\n",
    "The following is a simplified version of the generated `kernel.cpp` HLS code and the corresponding datapath diagram.\n",
    "\n",
    "<div style=\"text-align:center\"><img width=90% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1740727380/reorder_buffer_at_ccst24.png\" alt=\"scalar-vector\"></div>\n",
    "\n",
    "From the above generated code, we can see that Allo automatically creates an intermediate buffer for C and attach it inside the `i` loop. Also two additional loop nested named `j_init` and `j_back` are created to initialize and write the intermediate buffer back to output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<details><summary markdown=\"span\">Let's see some HLS report!</summary>\n",
    "\n",
    "```text\n",
    "+ Latency: \n",
    "    * Summary: \n",
    "    +---------+---------+----------+----------+---------+---------+---------+\n",
    "    |  Latency (cycles) |  Latency (absolute) |      Interval     | Pipeline|\n",
    "    |   min   |   max   |    min   |    max   |   min   |   max   |   Type  |\n",
    "    +---------+---------+----------+----------+---------+---------+---------+\n",
    "    |  2198686|  2198686|  7.322 ms|  7.322 ms|  2198687|  2198687|       no|\n",
    "    +---------+---------+----------+----------+---------+---------+---------+\n",
    "\n",
    "================================================================\n",
    "== Utilization Estimates\n",
    "================================================================\n",
    "* Summary: \n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|         Name        | BRAM_18K|  DSP |    FF   |   LUT   | URAM|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|DSP                  |        -|     -|        -|        -|    -|\n",
    "|Expression           |        -|     -|        0|      371|    -|\n",
    "|FIFO                 |        -|     -|        -|        -|    -|\n",
    "|Instance             |        0|     5|     3384|     4619|    0|\n",
    "|Memory               |       48|     -|       32|       65|    0|\n",
    "|Multiplexer          |        -|     -|        -|      668|    -|\n",
    "|Register             |        -|     -|      649|        -|    -|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Total                |       48|     5|     4065|     5723|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Available SLR        |     1344|  3008|   869120|   434560|  320|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Utilization SLR (%)  |        3|    ~0|       ~0|        1|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Available            |     4032|  9024|  2607360|  1303680|  960|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Utilization (%)      |        1|    ~0|       ~0|       ~0|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We have improved the total latency from 14,958,622 cycles to 2,198,686 cycles, a 6.8x speedup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check pipline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```text\n",
    "* Loop: \n",
    "+-----------------+---------+---------+----------+-----------+-----------+-------+----------+\n",
    "|                 |  Latency (cycles) | Iteration|  Initiation Interval  |  Trip |          |\n",
    "|    Loop Name    |   min   |   max   |  Latency |  achieved |   target  | Count | Pipelined|\n",
    "+-----------------+---------+---------+----------+-----------+-----------+-------+----------+\n",
    "|- l_S_k_0_k_l_j  |    16397|    16397|        15|          1|          1|  16384|       yes|\n",
    "+-----------------+---------+---------+----------+-----------+-----------+-------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `j` loop is pipelined to an initiation interval of 1, meaning that a new iteration can begin every cycle, achieving the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can increase parallelism by unrolling the `j` loop.\n",
    "\n",
    "**Exercise**: unroll loop `j` with a factor of 16.\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "unroll(axis[, factor])\n",
    "# Unrolls a loop with loop index axis by factor.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With unrolling, we create 16 parallel units to compute the scalar-vector product. The following is the datapath diagram of the unrolled design.\n",
    "\n",
    "<div style=\"text-align:center\"><img width=40% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1740727380/unroll_yl4lxg.png\" alt=\"scalar-vector-unroll\"></div>\n",
    "\n",
    "We push the design through synthesis again and observe the speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = s.build(target=\"vitis_hls\", mode=\"csyn\", project=\"unroll-scalar-vector.prj\")\n",
    "mod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HLS report is available in `unroll-scalar-vector.prj/out.prj/solution1/syn/report/gemm_csynth.rpt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<details><summary markdown=\"span\">Let's see some HLS report!</summary>\n",
    "\n",
    "```text\n",
    "+ Latency: \n",
    "+---------+---------+----------+----------+--------+--------+---------+\n",
    "|  Latency (cycles) |  Latency (absolute) |     Interval    | Pipeline|\n",
    "|   min   |   max   |    min   |    max   |   min  |   max  |   Type  |\n",
    "+---------+---------+----------+----------+--------+--------+---------+\n",
    "|   232478|   232478|  0.774 ms|  0.774 ms|  232479|  232479|       no|\n",
    "+---------+---------+----------+----------+--------+--------+---------+\n",
    "\n",
    "================================================================\n",
    "== Utilization Estimates\n",
    "================================================================\n",
    "* Summary: \n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|         Name        | BRAM_18K|  DSP |    FF   |   LUT   | URAM|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|DSP                  |        -|     -|        -|        -|    -|\n",
    "|Expression           |        -|     -|        0|      350|    -|\n",
    "|FIFO                 |        -|     -|        -|        -|    -|\n",
    "|Instance             |        0|    80|    12274|     9303|    0|\n",
    "|Memory               |       64|     -|      512|      528|    0|\n",
    "|Multiplexer          |        -|     -|        -|     2082|    -|\n",
    "|Register             |        -|     -|      634|        -|    -|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Total                |       64|    80|    13420|    12263|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Available SLR        |     1344|  3008|   869120|   434560|  320|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Utilization SLR (%)  |        4|     2|        1|        2|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Available            |     4032|  9024|  2607360|  1303680|  960|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "|Utilization (%)      |        1|    ~0|       ~0|       ~0|    0|\n",
    "+---------------------+---------+------+---------+---------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With unrolling, we created 16 parallel units to compute the scalar-vector product, so we use more DSPs and FFs. We further improve the total latency from 14,958,622 cycles to 232,478 cycles, a 64.4x total speedup! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the pipelined `j` loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```text\n",
    "* Loop: \n",
    "+-----------------+---------+---------+----------+-----------+-----------+------+----------+\n",
    "|                 |  Latency (cycles) | Iteration|  Initiation Interval  | Trip |          |\n",
    "|    Loop Name    |   min   |   max   |  Latency |  achieved |   target  | Count| Pipelined|\n",
    "+-----------------+---------+---------+----------+-----------+-----------+------+----------+\n",
    "|- l_S_k_0_k_l_j  |     1035|     1035|        13|          1|          1|  1024|       yes|\n",
    "+-----------------+---------+---------+----------+-----------+-----------+------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `j` loop is pipelined to an initiation interval of 1, meaning that a new iteration can begin every cycle, achieving the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we walked through the following topics:\n",
    "- Creating a customization schedule from an algorithm.\n",
    "- Using CPU backend for functional validation.\n",
    "- Targeting Vitis HLS for hardware synthesis.\n",
    "- Applying loop reordering, buffer insertion, pipelining, and unrolling to improve the performance.\n",
    "\n",
    "We have successfully transformed the matrix multiply example into an accelerator design with a 64.4x speedup! ðŸŽ‰\n",
    "\n",
    "In the next section, we will show how to _verify_ the correctness of the accelerator design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Each customization transform the Allo program. How to make sure the accelerator remains correct after applying hardware customizations? ðŸ¤”\n",
    "\n",
    "Allo integrates an equivalence verification tool that checks the equivalence before and after customizations:\n",
    "\n",
    "<div style=\"text-align:center\"><img width=60% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1739966977/verify_wa2lf4.png\" alt=\"verify\"></div>\n",
    "\n",
    "This verification tool interprets the program before and after customizations to build a pair of symbolic representations, then checks if they are equivalent. This method is agnostic to loop transformations, data layout, buffer insertion. If the two programs are equivalent, the verification tool will return `True`. Otherwise, it will return `False`, and give the difference between the two programs.\n",
    "\n",
    "To read more about the verification tool, please refer to our paper published at FPGA 2024: [Formal Verification of Source-to-Source Transformations for HLS](https://dl.acm.org/doi/10.1145/3626202.3637563).\n",
    "\n",
    "We verify the scalar-vector matrix multiply example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, K = 32, 32, 32\n",
    "\n",
    "def gemm(A: float32[M, K], B: float32[K, N]) -> float32[M, N]:\n",
    "    C: float32[M, N] = 0.0\n",
    "    for i, j in allo.grid(M, N):\n",
    "        for k in allo.reduction(K):\n",
    "            C[i, j] += A[i, k] * B[k, j]\n",
    "    return C\n",
    "\n",
    "s = allo.customize(gemm)\n",
    "s.reorder(\"k\", \"j\")\n",
    "s.buffer_at(s.C, axis=\"i\")\n",
    "s.pipeline(\"j\")\n",
    "s.unroll(\"j\", 4)\n",
    "\n",
    "\n",
    "s1 = allo.customize(gemm)\n",
    "equivalent = allo.verify(s, s1)\n",
    "if equivalent:\n",
    "    print(\"\\033[92m\" + \"Verification Passed!\" + \"\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[91m\" + \"Verification Failed!\" + \"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡**Tip**: You can verify the equivalence of two schedules at any point of composing the customizations, even at every step. This makes the verification process scalable to large-scale and complex customizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Allo, we can also easily convert a PyTorch model into an accelerator design.\n",
    "\n",
    "\n",
    "In Allo example directory, we provide demo code for converting a self-attention module, a Bert layer, and a full GPT2 model into accelerator designs. If you are interested in the details, please refer to the [examples](https://github.com/cornell-zhang/allo/tree/main/examples/torch) directory.\n",
    "\n",
    "Here, we show a small example of converting a simple MLP into an accelerator design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import allo\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(16, 32)  # 8*16 * 32*16\n",
    "        self.linear2 = torch.nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.linear1(data)\n",
    "        out = self.linear2(out)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = MLP()\n",
    "model.eval()\n",
    "example_inputs = [torch.rand(8, 16)]\n",
    "hls_mod = allo.frontend.from_pytorch(\n",
    "    model, example_inputs=example_inputs, verbose=False,\n",
    "    target=\"vitis_hls\", mode=\"csyn\", project=\"pytorch_demo.prj\"\n",
    ")\n",
    "hls_mod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Kernel Composition Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allo is a composable accelerator design framework with the ability to compose multiple kernels into a larger accelerator design.\n",
    "\n",
    "First, we show how to design a systolic array with stream type and spatial composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stream Types\n",
    "\n",
    "Spatial composition typically involves specializing distinct PEs for specific operators or layers, enabling direct communication between them using streaming buffers (e.g., FIFOs or multi-buffers). Similar to partition types, a stream can be viewed as a layout that enforces the memory access order. To improve spatial composability, we introduce the stream type, which serializes the data within it. As shown in the following figure, two operations are associated with the stream type: the `.put()`\n",
    "operation places data into the stream, while the `.get()` operation retrieves data from the stream in a first-in-first-out manner.\n",
    "\n",
    "<div style=\"text-align:center\"><img width=60% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1739966977/stream_type_arn3y3.png\" alt=\"stream_type\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing A Systolic Array with Spatial PE Composition\n",
    "\n",
    "A systolic array is a grid of processing elements (PEs) where each PE operates on the data it receives from its neighbors. The PEs are connected to each other through a network of communication channels, which allows them to exchange data and perform computations in parallel.\n",
    "\n",
    "<div style=\"text-align:center\"><img width=40% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1739961848/systolic_array_pix4ue.png\" alt=\"systolic_array\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allo.dataflow as df\n",
    "import numpy as np\n",
    "from allo.ir.types import float32\n",
    "\n",
    "\n",
    "M, N, K = 2, 2, 2\n",
    "P0, P1 = M + 2, N + 2\n",
    "\n",
    "@df.region()\n",
    "def top():\n",
    "    fifo_A = df.array(df.pipe(dtype=float32, shape=(), depth=4), shape=(P0, P1))\n",
    "    fifo_B = df.array(df.pipe(dtype=float32, shape=(), depth=4), shape=(P0, P1))\n",
    "\n",
    "    @df.kernel(mapping=[P0, P1])\n",
    "    def gemm(A: float32[M, K], B: float32[K, N], C: float32[M, N]):\n",
    "        i, j = df.get_pid()\n",
    "        # periperals kernels\n",
    "        with allo.meta_if(i in {0, M + 1} and j in {0, N + 1}):\n",
    "            pass\n",
    "        with allo.meta_elif(j == 0):\n",
    "            # i > 0\n",
    "            for k in range(K):\n",
    "                fifo_A[i, j + 1].put(A[i - 1, k])\n",
    "        with allo.meta_elif(i == 0):\n",
    "            # j > 0\n",
    "            for k in range(K):\n",
    "                fifo_B[i + 1, j].put(B[k, j - 1])\n",
    "        # drain\n",
    "        with allo.meta_elif(i == M + 1 and j > 0):\n",
    "            for k in range(K):\n",
    "                b: float32 = fifo_B[i, j].get()\n",
    "        with allo.meta_elif(j == N + 1 and i > 0):\n",
    "            for k in range(K):\n",
    "                a: float32 = fifo_A[i, j].get()\n",
    "        # main body\n",
    "        with allo.meta_else():\n",
    "            c: float32 = 0\n",
    "            for k in range(K):\n",
    "                a: float32 = fifo_A[i, j].get()\n",
    "                b: float32 = fifo_B[i, j].get()\n",
    "                c += a * b\n",
    "                fifo_A[i, j + 1].put(a)\n",
    "                fifo_B[i + 1, j].put(b)\n",
    "            C[i - 1, j - 1] = c\n",
    "\n",
    "\n",
    "A = np.random.rand(M, K).astype(np.float32)\n",
    "B = np.random.rand(K, N).astype(np.float32)\n",
    "C = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "sim_mod = df.build(top, target=\"simulator\")\n",
    "sim_mod(A, B, C)\n",
    "np.testing.assert_allclose(C, np.dot(A, B), atol=1e-5)\n",
    "print(\"\\033[92mDataflow Simulator Passed!\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing Two Systolic Arrays\n",
    "\n",
    "When composing multiple kernels, there could be conflicts in the data layout. Allo checks and resolves this by modeling data layout as types, and use type inference to check and fix such potential data layout inconsistency.\n",
    "\n",
    "In the following example, we show how to compose two systolic arrays with potential data layout inconsistency.\n",
    "\n",
    "<div style=\"text-align:center\"><img width=40% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1739969013/temporal_compose_xghz28.png\" alt=\"systolic_array\"></div>\n",
    "\n",
    "As shown in the above figure, we demonstrate an example of calling two consecutive GEMM kernels. By using the `.compose()` primitive, users can easily integrate the schedule of a subfunction into the top-level function.\n",
    "\n",
    "Here, we customize the submodule `systolic_tile`, then compose it with the top-level module.\n",
    "\n",
    "Notice the potential data layout inconsistency for tensor `Y`:\n",
    "\n",
    "- It is fully partitioned as the output of the first `systolic_tile` call.\n",
    "- It is also partitioned along rows as the input of the second `systolic_tile` call.\n",
    "\n",
    "To make sure the data layout is consistent, we fully partition both input A and output C.\n",
    "\n",
    "Allo models data layout as types, and use type inference to check and fix such potential data layout inconsistency. The intuition is, always supply equal or more parallelism, but never less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allo.library.systolic import systolic_tile\n",
    "from allo.ir.types import int8\n",
    "\n",
    "M0, M1, KK = 4, 4, 4\n",
    "W_A_cst = np.random.randint(-4, 4, size=(M0, M1)).astype(np.int8)\n",
    "W_B_cst = np.random.randint(-4, 4, size=(M0, M1)).astype(np.int8)\n",
    "\n",
    "def top(X: int8[M0, M1]) -> int8[M0, M1]:\n",
    "    Y: int8[M0, M1] = 0\n",
    "    Z: int8[M0, M1] = 0\n",
    "    W_A: int8[M0, M1] = W_A_cst\n",
    "    W_B: int8[M0, M1] = W_B_cst\n",
    "    systolic_tile[int8, int8, int8, KK, M0, M1](X, W_A, Y)\n",
    "    systolic_tile[int8, int8, int8, KK, M0, M1](Y, W_B, Z)\n",
    "    return Z\n",
    "\n",
    "s_top = allo.customize(top)\n",
    "# print(s_top.module)\n",
    "# CPU testing\n",
    "mod = s_top.build()\n",
    "X = np.random.randint(-4, 4, size=(M0, M1)).astype(np.int8)\n",
    "allo_C = mod(X)\n",
    "np_C = X @ W_A_cst @ W_B_cst\n",
    "np.testing.assert_allclose(allo_C, np_C, atol=1e-3)\n",
    "print(\"Passed!\")\n",
    "# Submodule customization\n",
    "s = allo.customize(\n",
    "    systolic_tile,\n",
    "    instantiate=[int8, int8, int8, KK, M0, M1],\n",
    ")\n",
    "s.partition(s.C, dim=0) \n",
    "s.partition(s.A, dim=1)\n",
    "s.partition(s.B, dim=2)\n",
    "pe = s.unfold(\"PE\", [0, 1])  # specify which are spatial loops\n",
    "s.to(s.A_fifo, pe, axis=1, depth=M0 + 1)\n",
    "s.to(s.B_fifo, pe, axis=0, depth=M1 + 1)\n",
    "# Compose with submodule\n",
    "s_top.compose(s)\n",
    "# HLS testing\n",
    "code = s_top.build(\"vhls\")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this tutorial, we demonstrated Allo's _verifiable_ accelerator design approach. We walked through the following topics:\n",
    "- Single kernel design and customization with Allo.\n",
    "- Verifying the correctness of the accelerator design.\n",
    "- Importing PyTorch models and converting them into accelerator designs.\n",
    "- Composing multiple kernels into a larger accelerator design.\n",
    "- Synthesis, simulation of the accelerator design targeting FPGA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, back to Hongzheng for AI Engine Demo!\n",
    "\n",
    "<div style=\"text-align:center\"><img width=90% src=\"https://res.cloudinary.com/dxzx2bxch/image/upload/v1740771896/aie_bmulwc.png\" alt=\"AIE\"></div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
